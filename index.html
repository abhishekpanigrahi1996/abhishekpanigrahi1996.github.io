<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Abhishek Panigrahi</title>

    <meta name="author" content="Abhishek Panigrahi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Abhishek Panigrahi
                </p>
                <p>
                  I am a fifth year graduate student in the Computer Science department at Princeton University, fortunate to be advised by Prof. Sanjeev Arora. Previously, I was a Research Fellow at Microsoft Research Lab - India where I worked with Dr. Harsha Vardhan Simhadri and Dr. Navin Goyal. Prior to the fellowship, I attended IIT Kharagpur where I obtained my B.Tech. in Computer Science and Engineering in 2018.
                </p>
                <p>
                  I am an <strong>Apple AI/ML Ph.D. scholar</strong> for the year 2025-26.
                </p>
                <p style="text-align:center">
                  <a href="mailto:ap34@princeton.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=oMhp8p8AAAAJ&hl=en&authuser=1">Scholar</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/image.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/image.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My current research focuses on mathematical models for efficient and robust training of language models. My works have spanned across different research directions, like neural tangent kernels, feature learning, implicit bias of optimization algorithms, skill localization in large language models, efficient pre-training, and test time adaptive architectures.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          </tbody></table>

					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Representative Works</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
          <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">On the Power of Context-Enhanced Learning in LLMs</span>
                <br>
                Xingyu Zhu*, <strong>Abhishek Panigrahi</strong>*, Sanjeev Arora
                <br>
                <em>In submission</em>
                <br>
                <a href="https://arxiv.org/abs/2503.01821">arXiv</a>
                /
                <a href="https://github.com/princeton-pli/Context-Enhanced-Learning">code</a>
                <p></p>
              </td>
          </tr>


            <tr>
                <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?
                    </span>
                  <br>
                  Simon Park*, <strong>Abhishek Panigrahi</strong>*, Catherine Cheng*, Dingli Yu, Anirudh Goyal, Sanjeev Arora
                  <br>
                  <em>In submission</em>
                  <br>
                  <a href="https://arxiv.org/abs/2501.02669">arXiv</a>
                  /
                  <a href="https://github.com/princeton-pli/VLM_S2H">code</a>
                  <p></p>
                </td>
            </tr>
          
          <tr>
            <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Progressive distillation induces an implicit curriculum</span>
              <br>
               <strong>Abhishek Panigrahi</strong>*, Bingbin Liu*, Sadhika Malladi, Andrej Risteski, Surbhi Goel
              <br>
              <em>International Conference on Learning Representations (ICLR 2025)</em> <span style="color:red;">(Oral)</span>
              <br>
              <a href="https://openreview.net/forum?id=wPMRwmytZe">Openreview</a>
              /
              <a href="https://github.com/abhishekpanigrahi1996/ProgressiveDistillation">code</a>
              /
              <a href="https://unprovenalgos.github.io/progressive-distillation">blog</a>
              <p></p>
            </td>
        </tr>

        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Efficient stagewise pretraining via progressive subnetworks</span>
            <br>
             <strong>Abhishek Panigrahi</strong>*, Nikunj Saunshi*, Kaifeng Lyu, Sobhan Miryoosefi, Sashank Reddi, Satyen Kale, Sanjiv Kumar
            <br>
            <em> International Conference on Learning Representations (ICLR 2025)</em> 
            <br>
            <a href="https://openreview.net/forum?id=rkgfdeBYvH">Openreview</a>
            <p></p>
          </td>
        </tr>


        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Task-specific skill localization in fine-tuned language models</span>
            <br>
             <strong>Abhishek Panigrahi</strong>*, Nikunj Saunshi*, Haoyu Zhao, Sanjeev Arora
            <br>
            <em>International Conference of Machine Learning (ICML 2023)</em>
            <br>
            <a href="https://proceedings.mlr.press/v202/panigrahi23a.html">PMLR</a>
            /
            <a href="https://github.com/abhishekpanigrahi1996/Skill-Localization-by-grafting">code</a>
            <p></p>
          </td>
      </tr>

        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Understanding Gradient Descent on the Edge of Stability in Deep Learning</span>
            <br>
             Sanjeev Arora, Zhiyuan Li, <strong>Abhishek Panigrahi</strong> (alphabetical)
            <br>
            <em>International Conference of Machine Learning (2022)</em> 
            <br>
            <a href="https://proceedings.mlr.press/v162/arora22a.html">PMLR</a>
            /
            <a href="https://github.com/abhishekpanigrahi1996/Edge_of_Stability">code</a>
            <p></p>
          </td>
        </tr>


      </tbody></table>



      <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
        <tr>
          <td>
            <h2>Other representative works</h2>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>

        
        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Trainable Transformer in Transformer</span>
            <br>
             <strong>Abhishek Panigrahi</strong>*, Sadhika Malladi*, Mengzhou Xia, Sanjeev Arora
            <br>
            <em> International Conference of Machine Learning (ICML 2024)</em> 
            <br>
            <a href="https://openreview.net/forum?id=JcxlFe2fGC">Openreview</a>
            /
            <a href="https://github.com/abhishekpanigrahi1996/transformer_in_transformer">Code</a>
            <p></p>
          </td>
        </tr>

        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Do transformers parse while predicting the masked word?</span>
            <br>
             Haoyu Zhao*, <strong>Abhishek Panigrahi</strong>*, Rong Ge, Sanjeev Arora
            <br>
            <em> Empirical Methods in Natural Language Processing (EMNLP 2023)</em> 
            <br>
            <a href="https://arxiv.org/abs/2303.08117">Arxiv</a>
            <p></p>
          </td>
        </tr>

        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Effect of Activation Functions on the Training of Overparametrized Neural Nets</span>
            <br>
             <strong>Abhishek Panigrahi</strong>*, Abhishek Shetty*, Navin Goyal
            <br>
            <em> International Conference on Learning Representations (ICLR 2020)</em> 
            <br>
            <a href="https://openreview.net/forum?id=rkgfdeBYvH">Openreview</a>
            <p></p>
          </td>
        </tr>


        <tr>
          <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Word2Sense: Sparse interpretable word embeddings</span>
            <br>
              <strong>Abhishek Panigrahi</strong>, Harsha Vardhan Simhadri, Chiranjib Bhattacharyya
            <br>
            <em> Association for Computational Linguistics (ACL 2019)</em>  <span style="color:red;">(Oral)</span>
            <br>
            <a href="https://aclanthology.org/P19-1570/">ACL</a>
            /
            <a href="https://github.com/abhishekpanigrahi1996/Word2Sense">code</a>
            <p></p>
          </td>
        </tr>

        </td>
      </tr>
    </table>
  </body>
</html>
