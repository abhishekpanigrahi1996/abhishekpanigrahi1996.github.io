---
title: "Analysis on Gradient Propagation in Batch Normalized Residual Networks"
collection: projects
type: "Summer Internship Project"
permalink: /projects/2019-10-03-project-1
location: "Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California"
date: 2017-07-12
---

We conduct mathematical analysis on the effect of batch normalization (BN) on gradient backpropogation in residual network training, which is believed to play a critical role in addressing the gradient vanishing/explosion problem, in this work. By analyzing the mean and variance behavior of the input and the gradient in the forward and backward passes through the BN and residual branches, respectively, we show that they work together to confine the gradient variance to a certain range across residual blocks in backpropagation. As a result, the gradient vanishing/explosion problem is avoided. We also show the relative importance of batch normalization wrt the residual branches in residual networks.

[Preprint Link](https://arxiv.org/pdf/1812.00342.pdf){:target="_blank"}.